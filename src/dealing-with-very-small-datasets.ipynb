{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "86c7b637c8f0b16fdd573ac8ff6576ab82f47cdd"
   },
   "source": [
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/creativity.jpg)\n",
    "\n",
    "# Dealing with very small datasets\n",
    "\n",
    "In this kernel we will see some techniques to handle very small datasets, where the main challenge is to avoid overfitting.\n",
    "\n",
    "1. <a href=\"#t1\">Why small datasets lead to overfitting?</a>\n",
    "2. <a href=\"#t2\">Use simple models</a>\n",
    "3. <a href=\"#t3\">Beware the outliers</a>\n",
    "4. <a href=\"#t4\">Select the features</a>\n",
    "5. <a href=\"#t5\">Balance the dataset with synthetic samples (SMOTE)</a>\n",
    "6. <a href=\"#t6\">Combine models for the final submission</a>\n",
    "7. <a href=\"#t7\">References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c26cdecd580c5993bef9df0d694918a7151cd6e5"
   },
   "source": [
    "Let's load the data from the **Don't Overfit! II** competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Z:/Lisa/IV/ds1_test_names.txt', 'r')\n",
    "ds_test_names = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "f = open('Z:/Lisa/IV/ds1_train_names.txt', 'r')\n",
    "ds_train_names = f.read().split('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv('Z:/Lisa/IV/features_CRC_T1w_WM.csv')\n",
    "ds.set_index('sub', inplace = True)\n",
    "train = ds.reindex(ds_train_names)\n",
    "test = ds.reindex(ds_test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train.columns.drop(['outcome'])\n",
    "target = train['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 56\n",
      "Test rows: 15\n",
      "Data columns: 107\n"
     ]
    }
   ],
   "source": [
    "print('Train rows:', train.shape[0])\n",
    "print('Test rows:', test.shape[0])\n",
    "print('Data columns:', test.columns.drop(['outcome']).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d051facc3f1f4492636a0a8fab86b395b593c146"
   },
   "source": [
    "A visual inspection of the number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6fd9e2a4a61f803057c2bc58c217a51968e56165"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d_names = ('train', 'test')\n",
    "y_pos = range(len(d_names))\n",
    " \n",
    "plt.bar(\n",
    "    y_pos, \n",
    "    (train.shape[0], test.shape[0]), \n",
    "    align='center', \n",
    "    alpha=0.8\n",
    ")\n",
    "plt.xticks(y_pos, d_names)\n",
    "plt.ylabel('Number of rows') \n",
    "plt.title('Wow!')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a45eede83e544b588fe660c3cf8b77e23ef3ed75"
   },
   "source": [
    "<h1 id=\"t1\">1. Why small datasets lead to overfitting?</h1>\n",
    "\n",
    "The goal of a machine learning model is to **generalize** patterns in training data so that you can correctly predict new data that has never been presented to the model. Overfitting occurs when a model adjusts excessively to the training data, seeing patterns that do not exist, and consequently performing poorly in predicting new data:\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/under_over.png)\n",
    "<center><strong>Source:</strong> <a href=\"https://medium.com/@shubhapatnim86/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5\">https://medium.com/@shubhapatnim86/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The fewer samples for training, the more models can fit our data. In an extreme example (a), for just one training point, any model will be able to \"explain\" it, however simple or complex the model may be. As we get to have more samples (b, c), fewer models are able to explain them:\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/few_samples.png)\n",
    "<center><strong>Source:</strong> <a href=\"https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d\">https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "That way, for a dataset with only 250 samples, we need to be very careful not to be fooled by overfitting. In this kernel we will see some tips that can help.\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/meme1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c86e4397232fa5e9ca6033629233a669dd4ca47"
   },
   "source": [
    "<h1 id=\"t2\">2. Use simple models</h1>\n",
    "\n",
    "As we saw, few samples allow several models to \"explain\" the data. By thinking graphically, complex models can make crazy curves that will almost perfectly explain the training data, but possibly will perform poorly over the test data.\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/overfitting_curve.png)\n",
    "<center><strong>Source:</strong> <a href=\"https://bioinfo.iric.ca/overfitting-and-regularization/\">https://bioinfo.iric.ca/overfitting-and-regularization/</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Avoid complex models with many parameters, thus limiting their generalization and possibility of overfitting. Regularization techniques like L1 and L2 also help make the models more conservative. For tree-based models, reducing their maximum depth also limits the model's ability to see patterns and non-existent relationships.\n",
    "\n",
    "A good model to get started is Logistic Regression, a linear model used when the dependent variable (target) is categorical (classification tasks).\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/logisticregression.png)\n",
    "<center><strong>Source:</strong> <a href=\"https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\">https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this model, we can control the regularization by means of the <code>penalty</code> and <code>C</code> parameters (inverse of regularization strength - smaller values specify stronger regularization) to deal with the overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "af6bee68c72fc1f24980cdf910b4af276523cab5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.84344263e-02, 9.83077935e-01, 9.37817162e-01, 2.16815392e-02,\n",
       "       1.41701067e-05, 9.99999269e-01, 9.99999765e-01, 2.18381698e-02,\n",
       "       1.00000000e+00, 5.63389188e-02, 7.68841159e-02, 9.99666176e-01,\n",
       "       1.50129216e-02, 9.86179613e-01, 8.23299570e-02])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "m = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=0.1\n",
    ")\n",
    "m.fit(train[labels], target)\n",
    "m.predict_proba(test[labels])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cc6dd4980e4aacbd3e42ea806bab09839175851"
   },
   "source": [
    "For tree-based models like XGBoost, we can control the overfitting by tuning a series of parameters:\n",
    "\n",
    "- Restricting the maximum depth of trees via <code>max_depth</code> (low values)\n",
    "- Making the model more conservative via <code>gamma</code> and <code>eta</code> (high values)\n",
    "- L1 and L2 regularization via <code>reg_alpha</code> and <code>reg_lambda</code> (high values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall xgboost --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost==0.90\n",
      "  Downloading xgboost-0.90-py2.py3-none-win_amd64.whl (18.3 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from xgboost==0.90) (1.17.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from xgboost==0.90) (1.2.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.90\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "552cc02db45c39cc57cc674e6abb4495c55c8a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1112957 , 0.8577334 , 0.23896545, 0.6089032 , 0.13614476,\n",
       "       0.8709664 , 0.76819336, 0.6281126 , 0.78023547, 0.1112957 ,\n",
       "       0.1212354 , 0.8577334 , 0.22353396, 0.6089032 , 0.12339927],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "m = XGBClassifier(\n",
    "    max_depth=2,\n",
    "    gamma=2,\n",
    "    eta=0.8,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.5\n",
    ")\n",
    "m.fit(train[labels], target)\n",
    "m.predict_proba(test[labels])[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "371cee8539913c6d7e473640f136965ea53383ea"
   },
   "source": [
    "**Note that because it is a nonlinear model with several parameters, it tends to be more prone to overfitting than a simple linear model such as logistic regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52a6bc0d76ea6787dca7f5b0a79a5c5957c8d2c3"
   },
   "source": [
    "<h1 id=\"t3\">3. Beware the outliers</h1>\n",
    "\n",
    "Outliers are extreme values that fall a long way outside of the other observations. In a small dataset, the impact of an outlier can be much greater, since it will have a heavy weight for the model:\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/outlier.png)\n",
    "<center><strong>Source:</strong> <a href=\"https://becominghuman.ai/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7\">https://becominghuman.ai/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "The scikit-learn library has several implementations of outliers detection techniques:\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/outl_detection.png)\n",
    "<center><strong>Source:</strong> <a href=\"https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html\">https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's do an experiment with the <code>IsolationForest</code> technique, which uses random forests for efficient detection of outliers in high-dimensional datasets. From scikit-learn documentation:\n",
    "\n",
    "> The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "> \n",
    "> Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n",
    "> \n",
    "> This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/isolation_forest.jpg)\n",
    "<center><strong>Source:</strong> <a href=\"https://www.slideshare.net/mlvlc/l14-anomaly-detection\">https://www.slideshare.net/mlvlc/l14-anomaly-detection</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's score each sample of our dataset using a isolation forest (the lower, the more abnormal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "fa4e76a07168b3638050b762aa5cb9f5d9ebfe28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41646273 -0.42833656 -0.41541237 -0.39636716 -0.46641776 -0.4122929\n",
      " -0.65718912 -0.42276507 -0.52878082 -0.40100428 -0.48125616 -0.4272023\n",
      " -0.40041509 -0.51419541 -0.3970606  -0.47153863 -0.41590376 -0.41805114\n",
      " -0.40641619 -0.44150171 -0.57012287 -0.4243093  -0.46942108 -0.43302943\n",
      " -0.4928181  -0.4076361  -0.38872283 -0.41813845 -0.44765493 -0.41607619\n",
      " -0.39916356 -0.40732789 -0.41723406 -0.46264657 -0.49999013 -0.40619314\n",
      " -0.41823994 -0.45207313 -0.39508319 -0.49388903 -0.40927852 -0.47293037\n",
      " -0.48301076 -0.42222252 -0.39603377 -0.41596034 -0.45750104 -0.49518995\n",
      " -0.49424636 -0.42461132 -0.47111303 -0.43870522 -0.44265952 -0.41740937\n",
      " -0.44326908 -0.39628224]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isf = IsolationForest(n_jobs=-1, random_state=1)\n",
    "isf.fit(train[labels], train['outcome'])\n",
    "\n",
    "print(isf.score_samples(train[labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ddba95cb7e564206474a39ce5889c93159adaa38"
   },
   "source": [
    "Now, let's predict the outliers (1 for inliers, -1 for outliers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9f2055a4e158a9d3031b8ba92b1b8536862c50c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e.lavrova\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isf.predict(train[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d42c39fec2ae9aae3c27aa0035ea0061c74bfbb1"
   },
   "source": [
    "**Beware, with few samples, it becomes a challenge to adjust the algorithms to correctly identify the outliers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7a275c6a941a7b20b908f5cbd4b18d2d6766870"
   },
   "source": [
    "<h1 id=\"t4\">4. Select the features</h1>\n",
    "\n",
    "While removing outliers consists of deleting rows from the dataset, feature selection consists of deleting columns that do not contribute to the prediction. There is a wide variety of methods, such as analysis of its correlation with the target, importance analysis and recursive elimination.\n",
    "\n",
    "Let's see an example of how to identify the most relevant features for classification using a tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "f721b3aca3bd586586f6ef3956a2550bf24be06c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features:\n",
      "1. feature 24 (0.049774)\n",
      "2. feature 28 (0.046298)\n",
      "3. feature 34 (0.032804)\n",
      "4. feature 25 (0.031014)\n",
      "5. feature 82 (0.030434)\n",
      "6. feature 51 (0.030432)\n",
      "7. feature 81 (0.030130)\n",
      "8. feature 21 (0.026282)\n",
      "9. feature 26 (0.025797)\n",
      "10. feature 18 (0.024553)\n",
      "11. feature 4 (0.020178)\n",
      "12. feature 15 (0.018670)\n",
      "13. feature 12 (0.018552)\n",
      "14. feature 102 (0.018155)\n",
      "15. feature 19 (0.017600)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "TOP_FEATURES = 15\n",
    "\n",
    "forest = ExtraTreesClassifier(n_estimators=250, max_depth=5, random_state=1)\n",
    "forest.fit(train[labels], train['outcome'])\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std(\n",
    "    [tree.feature_importances_ for tree in forest.estimators_],\n",
    "    axis=0\n",
    ")\n",
    "indices = np.argsort(importances)[::-1]\n",
    "indices = indices[:TOP_FEATURES]\n",
    "\n",
    "print('Top features:')\n",
    "for f in range(TOP_FEATURES):\n",
    "    print('%d. feature %d (%f)' % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ecc50c16f2cf1c39bf6d5c1e198c59fb8e99b74"
   },
   "source": [
    "We can plot a chart of the importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "ac55b127995b43ed7347e59d9b8e9e07484cf802"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('Top feature importances')\n",
    "plt.bar(\n",
    "    range(TOP_FEATURES), \n",
    "    importances[indices],\n",
    "    yerr=std[indices], \n",
    ")\n",
    "plt.xticks(range(TOP_FEATURES), indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ea7fbeb73ee8e742bd5671d9050a295130d063e"
   },
   "source": [
    "In the next example, we will try to identify the most important features by successively training a model and recursively eliminating those that do not contribute to a good final solution (according to the selected model).\n",
    "\n",
    "We will use the recursive feature elimination <code>RFE</code> from the scikit-learn library and the <code>XGBClassifier</code> model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "adb2e9aa37a3064ef42fdd2bf8a3d6a6933be3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:\n",
      "['original_shape_Elongation', 'original_shape_Flatness', 'original_shape_LeastAxisLength', 'original_shape_MajorAxisLength', 'original_shape_Maximum2DDiameterColumn', 'original_shape_Maximum2DDiameterRow', 'original_shape_Maximum2DDiameterSlice', 'original_shape_Maximum3DDiameter', 'original_shape_MeshVolume', 'original_shape_MinorAxisLength', 'original_shape_Sphericity', 'original_shape_SurfaceArea', 'original_shape_SurfaceVolumeRatio', 'original_shape_VoxelVolume', 'original_firstorder_10Percentile', 'original_firstorder_90Percentile', 'original_firstorder_Energy', 'original_firstorder_InterquartileRange', 'original_firstorder_Maximum', 'original_firstorder_Median', 'original_firstorder_RootMeanSquared', 'original_firstorder_Skewness', 'original_firstorder_Uniformity', 'original_glcm_ClusterProminence', 'original_glcm_ClusterShade', 'original_glcm_ClusterTendency', 'original_glcm_Correlation', 'original_glcm_MCC', 'original_glrlm_GrayLevelNonUniformity', 'original_glrlm_GrayLevelNonUniformityNormalized', 'original_glrlm_GrayLevelVariance', 'original_glrlm_HighGrayLevelRunEmphasis', 'original_glrlm_LongRunHighGrayLevelEmphasis', 'original_glrlm_RunLengthNonUniformity', 'original_glrlm_RunPercentage', 'original_glrlm_RunVariance', 'original_glrlm_ShortRunEmphasis', 'original_glrlm_ShortRunHighGrayLevelEmphasis', 'original_glrlm_ShortRunLowGrayLevelEmphasis', 'original_glszm_GrayLevelNonUniformity', 'original_glszm_GrayLevelNonUniformityNormalized', 'original_glszm_GrayLevelVariance', 'original_glszm_HighGrayLevelZoneEmphasis', 'original_glszm_SmallAreaHighGrayLevelEmphasis', 'original_gldm_DependenceEntropy', 'original_gldm_DependenceNonUniformity', 'original_gldm_LargeDependenceLowGrayLevelEmphasis', 'original_gldm_LowGrayLevelEmphasis', 'original_gldm_SmallDependenceEmphasis', 'original_gldm_SmallDependenceHighGrayLevelEmphasis', 'original_gldm_SmallDependenceLowGrayLevelEmphasis', 'original_ngtdm_Busyness', 'original_ngtdm_Coarseness']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(XGBClassifier(n_jobs=-1, random_state=1))\n",
    "\n",
    "rfe.fit(train[labels], train['outcome'])\n",
    "\n",
    "print('Selected features:')\n",
    "print(labels[rfe.support_].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "627acfe6b68e64db435b37ebb9bfe45e412c2df1"
   },
   "source": [
    "**Caution: for a very small dataset, very robust local validation is required to determine whether or not a feature contributes to the final solution. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "deb0972159b6229a555db03f3efdc6d8cddd8e95"
   },
   "source": [
    "<h1 id=\"t5\">5. Balance the dataset with synthetic samples (SMOTE)</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's look at the distribution of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "5a2736f9c0d29dd6514c89d82c45be3e9a338ae9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['outcome'].value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e9632368a131cac585107c69662f8bdc9cd9b4c"
   },
   "source": [
    "In addition to being extremely small, our training dataset has the unbalanced <code>target</code> binary variable, which can undermine some models' predictability. We will perform an oversampling, which consists of creating new samples to increase the <code>0</code> minority class. For this we will use the SMOTE technique.\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)\n",
    "<center><strong>Source:</strong> <a href=\"https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\">https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "We'll use the SMOTE implementation from the library <code>imbalanced-learn</code>, with the parameter <code>ratio='minority'</code> to resample the minority class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fb6548d56956e994069d0641283fc17b534a910"
   },
   "source": [
    "<h1 id=\"t6\">6. Combine models for the final submission</h1>\n",
    "\n",
    "Combine the prediction of several models or the same model with different values of hyperparameters reduces variance and enhances generalization.\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/combine.jpg)\n",
    "<center><strong>Source:</strong> <a href=\"https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89\">https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Often, combining weak models that are poorly correlated with each other can lead to superior results than a strong individual model. There are several ways to do this. The simplest is to perform a weighted average of the various predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "6c94a9467b0cbbfdb2c65d1690233efd2bce1cf9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>weighted_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.159110</td>\n",
       "      <td>0.010114</td>\n",
       "      <td>0.124727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.634840</td>\n",
       "      <td>0.973739</td>\n",
       "      <td>0.713048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.155817</td>\n",
       "      <td>0.375654</td>\n",
       "      <td>0.206549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063190</td>\n",
       "      <td>0.639113</td>\n",
       "      <td>0.196095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.137654</td>\n",
       "      <td>0.089303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  weighted_pred\n",
       "0  0.159110  0.010114       0.124727\n",
       "1  0.634840  0.973739       0.713048\n",
       "2  0.155817  0.375654       0.206549\n",
       "3  0.063190  0.639113       0.196095\n",
       "4  0.074798  0.137654       0.089303"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    XGBClassifier(max_depth=2)\n",
    "]\n",
    "\n",
    "preds = pd.DataFrame()\n",
    "for i, m in enumerate(models):\n",
    "    m.fit(train[labels], target),\n",
    "    preds[i] = m.predict_proba(test[labels])[:,1]\n",
    "\n",
    "weights = [1, 0.3]\n",
    "preds['weighted_pred'] = (preds * weights).sum(axis=1) / sum(weights)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e0ebfeab0afddd15ebcb263878c6d8110db2d22"
   },
   "source": [
    "Another more sophisticated way of combining predictions is the use of a meta-classifier, which receives as input the prediction of other classifiers, and performs the final predict. From the <code>mlxtend</code> library documentation:\n",
    "\n",
    "> Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n",
    "\n",
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/stack.png)\n",
    "<center><strong>Source:</strong> <a href=\"http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\">http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/</a></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "As an example, we will use the <code>StackingClassifier</code> of the <code>mlxtend</code> library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Using cached mlxtend-0.17.2-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from mlxtend) (1.17.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from mlxtend) (1.0.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from mlxtend) (0.14.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from mlxtend) (3.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from mlxtend) (45.1.0.post20200127)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from mlxtend) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from mlxtend) (0.22.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2018.7)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2.7.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\e.lavrova\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->mlxtend) (1.12.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "90a2afb30f624565faa6d2f7a6e04220003b0938"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b86e9e550b85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m m = StackingClassifier(\n\u001b[0;32m      4\u001b[0m     classifiers=[\n\u001b[0;32m      5\u001b[0m         \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "m = StackingClassifier(\n",
    "    classifiers=[\n",
    "        LogisticRegression(),\n",
    "        XGBClassifier(max_depth=2)\n",
    "    ],\n",
    "    use_probas=True,\n",
    "    meta_classifier=LogisticRegression()\n",
    ")\n",
    "\n",
    "m.fit(train[labels], target),\n",
    "preds['stack_pred'] = m.predict_proba(test[labels])[:,1]\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04c653c3e5b8380b2e297be67865b516022dc6b5"
   },
   "source": [
    "![](https://raw.githubusercontent.com/rafjaa/curso-mineracao-de-dados-aplicada/master/img/kernel_overfitting/mem2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8a97a205f557f6cd475e067a8ee953ec5f52e87"
   },
   "source": [
    "<h1 id=\"t7\">References</h1>\n",
    "\n",
    "**Overfitting:**\n",
    "* IRIC's Bioinformatics Platform. Overfitting and Regularization. https://bioinfo.iric.ca/overfitting-and-regularization/\n",
    "* PATNI, Shubham. Generalisation, Training-Validation & Test data. Machine Learning- Part 6. https://medium.com/@shubhapatnim86/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5\n",
    "* Rants on Machine Learning. What to do with “small” data? https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89\n",
    "* Towards Data Science. Breaking the curse of small datasets in Machine Learning: Part 1. https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d\n",
    "\n",
    "**Models:**\n",
    "* scikit-learn. LogisticRegression. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* Towards Data Science. Logistic Regression — Detailed Overview. https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n",
    "* XGBoost. XGBoost Parameters. https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "**Outlier detection:**\n",
    "* Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.” Data Mining, 2008. ICDM‘08. Eighth IEEE International Conference on.\n",
    "* Machine Learning Valencia. Anomaly Detection. Valencian Summer School 2015. https://www.slideshare.net/mlvlc/l14-anomaly-detection\n",
    "* scikit-learn. Novelty and Outlier Detection. https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "* VADALI, SaiGayatri.Day 7: Data cleaning — All you need to know about it. https://becominghuman.ai/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7\n",
    "\n",
    "**Feature selection:**\n",
    "* scikit-learn. Feature selection. https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "**SMOTE**\n",
    "* ALENCAR, Rafael. Resampling strategies for imbalanced datasets. https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    "* Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002)\n",
    "* imbalanced-learn. https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "**Stacking**\n",
    "* mlxtend. StackingClassifier. http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\n",
    "* Tang, J., S. Alelyani, and H. Liu. \"Data Classification: Algorithms and Applications.\" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500.\n",
    "* Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2 (1992): 241-259."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
